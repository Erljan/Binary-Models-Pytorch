{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TORCH\n",
    "- Think of this as a toolbox for building and using neural networks\n",
    "\n",
    "#### torch.nn \n",
    "- Think of this as a special part of the toolbox with all the pieces you need to build brain-like models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Tools breakdown\n",
    "\n",
    "#### nn.Linear\n",
    "- This layer connects the input to the next layer by performing a math operation\n",
    "\n",
    "#### nn.Sigmoid\n",
    "- This is a function. What this function does is that it squashes the numbers to be between 0 and 1.\n",
    "- This is used to make decisions yes or no/true or false\n",
    "\n",
    "#### nn.Sequential\n",
    "- Think of this like stacking a lego block. Each block is a layer and you put each layer on top of the other in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=3, out_features=2, bias=True)\n",
      "  (1): Linear(in_features=2, out_features=1, bias=True)\n",
      "  (2): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Neurons/Nodes are like tiny decision makers, in the first layer we have 3 inputs (3 decision maker), then layer 2 has 2 inputs(2 decision maker), then the final output has 1(1 decision maker)\n",
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(3,2), # The 3 is the input, the 2 is then the output\n",
    "    nn.Linear(2,1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tensors\n",
    "- Tensor is like a list of numbers arrange in a specific way. Think of tensors like it's an array (but it's a multi dimensional array, so like arrays inside an array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([\n",
    "    [1. , 1. , 1.] # This creates a tensor with one row and three column all filled with 1.0 (1x3) TENSORS HAVE TO BE FLOATS!! OTHERWISE IT WONT ACCEPT\n",
    "\n",
    "])\n",
    "\n",
    "torch.tensor([\n",
    "    [0.1, 0.2, 0.3],  # This  creates a tensor with 5 rows and 3 columns (5x3)\n",
    "    [0.4, 0.5, 0.6],   # Although since I initialize at the top that it only accepts 3 inputs (3 neurons), if I make each layer to contain 4 neurons, it will be unacceptable\n",
    "    [0.7, 0.8, 0.9],\n",
    "    [1.0, 1.1, 1.2],\n",
    "    [1.3, 1.4, 1.5]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed a Tensor to our Model\n",
    "- Create a tensor with 3 columns and make sure it's a type float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tensor for our model\n",
    "input_tensor = torch.tensor([\n",
    "    [0.1, 0.2, 0.3]\n",
    "])\n",
    "\n",
    "#Ensure tensors are float since it cant accepts an integer\n",
    "input_tensor = input_tensor.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a simple model\n",
    "- nn.Sequential : THis is like stacking blocks in a row one after the other\n",
    "- nn.Linear(3,2): This block takes 3 inputs and turns them into 2 outputs.\n",
    "- nn.Linear(2,1): This block takes the 2 outputs from the previous layer and turns them into 1 output.\n",
    "- nn.Sigmoid(): This block squashes the final number between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model using sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(3,2), #Input layer to hidden layer\n",
    "    nn.Linear(2,1), #Hidden layer to output layer\n",
    "    nn.Sigmoid() #Activation layer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4561]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Running the model\n",
    "\n",
    "output = model(input_tensor)\n",
    "output # This will print it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and Biases\n",
    "- This is like deciding how much you like different kinds of fruits, you have scoring system where you give point to each fruit based on how much you like them\n",
    " #### Weights\n",
    " - Think of weights as \"Importance\" or \"Preference\" you give to each type of fruit\n",
    "   - If you really like Apples, you give apple a high weight or high importance\n",
    "   - If you don't like bananas, you give it a lower weight or low importance\n",
    " #### Biases\n",
    " - Now imagine that no matter what fruit it is, you give extra point because you just love fruits in general, this extra point is called 'bias'\n",
    " - You add the extra point after all the points in the weight is calculated\n",
    "   - Examples: \n",
    "      - Multiply each fruit by its weight:\n",
    "         - Apples: 2 (number of apples) * 3 (weight) = 6 points\n",
    "         - Bananas: 1 (number of bananas) * 1 (weight) = 1 point\n",
    "         - Cherries: 3 (number of cherries) * 2 (weight) = 6 points   --->> This will total to 13 points. Then you add the Bias ---> 13 + 1 (bias) = 14 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight tensor([[ 0.2197,  0.3834,  0.3737],\n",
      "        [ 0.2718,  0.2545, -0.1983]])\n",
      "0.bias tensor([-0.1764,  0.2548])\n",
      "1.weight tensor([[-0.0162, -0.2990]])\n",
      "1.bias tensor([-0.0936])\n"
     ]
    }
   ],
   "source": [
    "# The model.named_parameters() is a built-in function in pytorch from the nn.Module class. This returns an iterator that provides both the name and parameter (like weight and biases) of each later\n",
    "# if param.requires_grad: This line is like asking, \"Is this part of the model something we need to change and learn from?\" If the answer is yes, we'll look at it more closely. This checks a condition (something that can be true or false). This tells us if the current part (weights or biases) should be changed during learning.\n",
    "\n",
    "# View weights and biases\n",
    "for name, param in model.named_parameters(): \n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Biases example\n",
    " - Weights: These are numbers that the model uses to connect one layer to the next\n",
    " - Biases: These are extra numbers that help the mmodel make better predictions\n",
    "\n",
    " #### How they are applied from the code below\n",
    " - First layer: hidden_layer_output = input_tensor * first_layer_weights^T + first_layer_biases\n",
    " - Second Layer: output = hidden_layer_output * second_layer_weights^T + second_layer_biases\n",
    " - Final output: final_output = sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6352, -0.1661]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First layer weights:\n",
    "# These weight connect the 3 input features to the 2 neurons in the hidden layer\n",
    "tensor([[-0.4012, -0.5649,  0.3153],\n",
    "        [-0.4333,  0.0119, -0.4002]])\n",
    "\n",
    "# First layer biases:\n",
    "# These biases are added to the weighted sum of inputs for each neuron in the hidden layer\n",
    "tensor([-0.3478, -0.4386])\n",
    "\n",
    "# Using the input_tensor from above, this is how the weights and biases work for the first hidden layer:\n",
    "# hidden_layer_output[0] = (0.1 * -0.4012) + (0.2 * -0.5649) + (0.3 * 0.3153) - 0.3478\n",
    "#                        = -0.04012 - 0.11298 + 0.09459 - 0.3478\n",
    "#                        = -0.40631\n",
    "\n",
    "\n",
    "# Using the input_tensor from above, this is how the weights and biases work for the first hidden layer:\n",
    "# hidden_layer_output[1] = (0.1 * -0.4333) + (0.2 * 0.0119) + (0.3 * -0.4002) - 0.4386\n",
    "#                        = -0.04333 + 0.00238 - 0.12006 - 0.4386\n",
    "#                        = -0.59961\n",
    "\n",
    "# The -0.40631 AND -0.59961 are the output from the nn.Linear(3,2) in the input_tensor above\n",
    "\n",
    "\n",
    "# Second Layer weights:\n",
    "# these weights connect the 2 neurons in the hidden layer to the single output neuron\n",
    "tensor([[-0.6352, -0.1661]])\n",
    "\n",
    "# Second layer biases:\n",
    "# These bias is added to the weighted sum of inputs for the output neuron\n",
    "tensor([0.4511])\n",
    "\n",
    "# Then the calculation of the second layer which is nn.Linear(2,1) is:\n",
    "# output = (-0.40631 * -0.6352) + (-0.59961 * -0.1661) + 0.4511\n",
    "#        ≈ 0.25806 + 0.09957 + 0.4511\n",
    "#        ≈ 0.80873\n",
    "\n",
    "\n",
    "# Sigmoid Activation:\n",
    "# final_output = sigmoid(0.80873) ≈ 1 / (1 + e^(-0.80873)) ≈ 0.6918\n",
    "# Explanation:\n",
    "        # Sigmoid Function: The calculator uses a function called sigmoid.\n",
    "        # This function makes sure the number you get is between 0 and 1.\n",
    "        # It's like putting a number through a machine that squishes it to fit between 0 and 1.\n",
    "        # More explanation:\n",
    "                # The \"e\" represents Euler's number, which helps the sigmoid function turn a number (0.80873 in this case) into a value between 0 and 1,\n",
    "                # making it useful for decisions in things like artificial intelligence and other complex calculations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Weights and Biases Manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[0].weight = nn.Parameter(torch.tensor([[-0.5, 0.2, 0.1], [0.4, -0.1, -0.3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Model with nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5137]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BinaryModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(3,2)\n",
    "        self.linear2 = nn.Linear(2,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "model = BinaryModel()\n",
    "\n",
    "new_output = model(input_tensor)\n",
    "new_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
